{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgilos/Bert-for-text-classification/blob/main/Active%2BBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S67cOg4MtLIE",
        "outputId": "b6cfe555-cc37-48f5-ca1a-1df61e20052c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bert-for-text-classification'...\n",
            "remote: Enumerating objects: 569, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 569 (delta 19), reused 16 (delta 11), pack-reused 540 (from 2)\u001b[K\n",
            "Receiving objects: 100% (569/569), 123.07 MiB | 10.77 MiB/s, done.\n",
            "Resolving deltas: 100% (278/278), done.\n",
            "Updating files: 100% (35/35), done.\n"
          ]
        }
      ],
      "source": [
        "#cloning the repository\n",
        "#https://github.com/rmunro/pytorch_active_learning.git\n",
        "#https://github.com/georgilos/Bert-for-text-classification.git\n",
        "!git clone https://github.com/georgilos/Bert-for-text-classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rkxQlxalKN8",
        "outputId": "3ce02bf3-bc41-450b-e8d1-d969f3183f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mBert-for-text-classification\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jegydkELtWMO",
        "outputId": "03432320-3e67-459c-e093-4d8b235ecd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Bert-for-text-classification\n"
          ]
        }
      ],
      "source": [
        "#changing directory to repository\n",
        "#%cd pytorch_active_learning/\n",
        "%cd Bert-for-text-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3ihIG5S17HN",
        "outputId": "ff14d38e-58bd-40f6-fa88-5f58efd8251e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFECdj7lcz3"
      },
      "source": [
        "### Excecuting active_learning_basic.py with the SimpleTextClassifier swapped with bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n4OlALQcPZvK",
        "outputId": "8f065cb6-9d4e-46ab-85dc-d5a0cc02c8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Creating initial training data:\n",
            "\n",
            "284 more annotations needed\n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "i have found a website for the teacher s television channel\n",
            "\n",
            "> \n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "here is your comment high above yes if he thinks every crackpot notion that the regional fed banks being private go back to your village retard because the regional banks are in fact honest that misses you hmmmmmm on second thought no it doesn t or would ever miss you\n",
            "\n",
            "> 1\n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "@user cnn let’s see what other idiotic claims we can make using the same logic: more blacks = more dead cops more hispanics = more illiteracy more asians = more anti-asian racism at harvard\n",
            "\n",
            "> 1\n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "stupid bitch said shit go to hell\n",
            "\n",
            "> 1\n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "cable news 25-54 year old demo viewers mon mar 14 audience share 1⃣ foxnews 50% 2⃣ cnn 28% 3⃣ msnbc 18% 4⃣ @user 4% @user n/a\n",
            "\n",
            "> \n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "cnn what about stopping the invasion of the usa at the southern border\n",
            "\n",
            "> \n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "they love being white so glad i is not bad yourself for\n",
            "\n",
            "> \n",
            "Please type 1 if this message is hate-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-edf984fede2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-edf984fede2b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{needed} more annotations needed\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Print needed annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mannotated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mrelated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotated_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-edf984fede2b>\u001b[0m in \u001b[0;36mget_annotations\u001b[0;34m(data, default_sampling_strategy)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_instructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mlabel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlabel_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"INTRODUCTION TO ACTIVE LEARNING\n",
        "\n",
        "A simple text classification algorithm in PyTorch\n",
        "\n",
        "This is an open source example to accompany Chapter 2 from the book:\n",
        "\"Human-in-the-Loop Machine Learning\"\n",
        "\n",
        "This example tries to classify news headlines into one of two categories:\n",
        "  disaster-related\n",
        "  not disaster-related\n",
        "\n",
        "It looks for low confidence items and outliers humans should review\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "import datetime\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "from random import shuffle\n",
        "from collections import defaultdict\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW  # Use PyTorch's AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import logging\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "__author__ = \"Robert Munro\"\n",
        "__license__ = \"MIT\"\n",
        "__version__ = \"1.0.1\"\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# settings\n",
        "\n",
        "minimum_evaluation_items = 1198  # annotate this many randomly sampled items first for evaluation data before creating training data\n",
        "minimum_training_items = 400  # minimum number of training items before we first train a model\n",
        "\n",
        "epochs = 3  # number of epochs per training session\n",
        "select_per_epoch = 200  # (not used) number to select per epoch per label\n",
        "batch_size = 16  # Batch size for training\n",
        "\n",
        "data = []\n",
        "test_data = []\n",
        "\n",
        "# directories with data\n",
        "unlabeled_data = \"data/unlabeled_data/unlabeled_combined.csv\"\n",
        "evaluation_related_data = \"data_for_notebook_code/evaluation_data/related.csv\"\n",
        "evaluation_not_related_data = \"data_for_notebook_code/evaluation_data/not_related.csv\"\n",
        "training_related_data = \"data_for_notebook_code/training_data/related.csv\"\n",
        "training_not_related_data = \"data_for_notebook_code/training_data/not_related.csv\"\n",
        "# validation_related_data # not used in this example\n",
        "# validation_not_related_data # not used in this example\n",
        "\n",
        "already_labeled = {}  # tracking what is already labeled\n",
        "\n",
        "def load_data(filepath, skip_already_labeled=False):\n",
        "    # csv format: [ID, TEXT, LABEL, SAMPLING_STRATEGY, CONFIDENCE]\n",
        "    data = []\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            for row in reader:\n",
        "                if skip_already_labeled and row[0] in already_labeled:\n",
        "                    continue\n",
        "                # Ensure all necessary columns are present\n",
        "                while len(row) < 5:\n",
        "                    if len(row) == 2:\n",
        "                        row.append(\"\")  # LABEL\n",
        "                    elif len(row) == 3:\n",
        "                        row.append(\"\")  # SAMPLING_STRATEGY\n",
        "                    elif len(row) == 4:\n",
        "                        row.append(0)  # CONFIDENCE\n",
        "                data.append(row)\n",
        "                label = str(row[2])\n",
        "                if row[2] != \"\":\n",
        "                    textid = row[0]\n",
        "                    already_labeled[textid] = label\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {filepath}\")\n",
        "    return data\n",
        "\n",
        "def append_data(filepath, data):\n",
        "    \"\"\"Append data to a CSV file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'a', encoding='utf-8', errors='replace', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerows(data)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {filepath}\")\n",
        "\n",
        "def write_data(filepath, data):\n",
        "    \"\"\"Write data to a CSV file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8', errors='replace', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerows(data)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {filepath}\")\n",
        "\n",
        "# LOAD ALL UNLABELED, TRAINING, VALIDATION, AND EVALUATION DATA\n",
        "training_data = load_data(training_related_data) + load_data(training_not_related_data)\n",
        "training_count = len(training_data)\n",
        "\n",
        "evaluation_data = load_data(evaluation_related_data) + load_data(evaluation_not_related_data)\n",
        "evaluation_count = len(evaluation_data)\n",
        "\n",
        "data = load_data(unlabeled_data, skip_already_labeled=True)\n",
        "\n",
        "annotation_instructions = (\n",
        "    \"Please type 1 if this message is hate-related, \"\n",
        "    \"or hit Enter if not.\\n\"\n",
        "    \"Type 2 to go back to the last message, \"\n",
        "    \"type d to see detailed definitions, \"\n",
        "    \"or type s to save your annotations.\\n\"\n",
        ")\n",
        "\n",
        "last_instruction = (\n",
        "    \"All done!\\n\"\n",
        "    \"Type 2 to go back to change any labels,\\n\"\n",
        "    \"or Enter to save your annotations.\"\n",
        ")\n",
        "\n",
        "detailed_instructions = (\n",
        "    \"A 'hate-related' tweet is any sentence with foul language.\\n\"\n",
        "    \"It includes:\\n\"\n",
        "    \"  - offensive discourse targeting a group or an individual.\\n\"\n",
        "    \"  - use of slurs, based on inherent characteristics (race, religion or gender).\\n\"\n",
        "    \"  - glorification of war crimes.\\n\"\n",
        "    \"It does not include:\\n\"\n",
        "    \"  - Polite disagreements\\n\"\n",
        "    \"  - Non-agressive use of minority terms.\\n\\n\"\n",
        ")\n",
        "\n",
        "def get_annotations(data, default_sampling_strategy=\"random\"):\n",
        "    \"\"\"Prompts annotator for label from command line and adds annotations to data\n",
        "\n",
        "    Keyword arguments:\n",
        "        data -- a list of unlabeled items where each item is\n",
        "                [ID, TEXT, LABEL, SAMPLING_STRATEGY, CONFIDENCE]\n",
        "        default_sampling_strategy -- strategy to use for each item if not already specified\n",
        "    \"\"\"\n",
        "\n",
        "    ind = 0\n",
        "    while ind < len(data):\n",
        "        if ind < 0:\n",
        "            ind = 0  # in case you've gone back before the first\n",
        "        if ind < len(data):\n",
        "            textid = data[ind][0]\n",
        "            text = data[ind][1]\n",
        "            label = data[ind][2]\n",
        "            strategy = data[ind][3]\n",
        "\n",
        "            if textid in already_labeled:\n",
        "                print(f\"Skipping seen label: {label}\")\n",
        "                ind += 1\n",
        "            else:\n",
        "                print(annotation_instructions)\n",
        "                label_input = input(text + \"\\n\\n> \").strip()\n",
        "\n",
        "                if label_input == \"2\":\n",
        "                    ind -= 1  # go back\n",
        "                elif label_input == \"d\":\n",
        "                    print(detailed_instructions)  # print detailed instructions\n",
        "                elif label_input == \"s\":\n",
        "                    break  # save and exit\n",
        "                else:\n",
        "                    if label_input != \"1\":\n",
        "                        label = \"0\"  # treat everything other than 1 as 0\n",
        "                    else:\n",
        "                        label = \"1\"\n",
        "\n",
        "                    data[ind][2] = label  # add label to our data\n",
        "\n",
        "                    if not data[ind][3]:\n",
        "                        data[ind][3] = default_sampling_strategy  # add default if none given\n",
        "                    ind += 1\n",
        "\n",
        "        else:\n",
        "            # Last one - give annotator a chance to go back\n",
        "            print(last_instruction)\n",
        "            label_input = input(\"\\n\\n> \").strip()\n",
        "            if label_input == \"2\":\n",
        "                ind -= 1\n",
        "            else:\n",
        "                ind += 1\n",
        "\n",
        "    return data\n",
        "\n",
        "class BERTTextClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, max_seq_length):\n",
        "        super(BERTTextClassifier, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)  # Use the pooled output for classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_seq_length):\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "        self.labels = []\n",
        "\n",
        "        for item in data:\n",
        "            try:\n",
        "                label = int(item[2])\n",
        "            except ValueError:\n",
        "                # If not, skip this item or assign a default label\n",
        "                logger.warning(f\"Skipping item with invalid label: {item[2]}\")  # Log the warning\n",
        "                print(f\"Skipping item with invalid label: {item[2]}\")  # Print to console\n",
        "                continue  # Skip this item\n",
        "            text = item[1]\n",
        "            encoded_input = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_seq_length,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            # Avoid using torch.tensor on existing tensors\n",
        "            self.input_ids.append(encoded_input['input_ids'].clone().detach().squeeze(0))\n",
        "            self.attention_masks.append(encoded_input['attention_mask'].clone().detach().squeeze(0))\n",
        "            self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "def train_model(training_data, validation_data, evaluation_data, num_labels=2):\n",
        "    \"\"\"Train model on the given training_data\n",
        "      Tune with the validation_data\n",
        "      Evaluate accuracy with the evaluation_data\n",
        "    \"\"\"\n",
        "    model = BERTTextClassifier(num_labels=num_labels, max_seq_length=128).to(device)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    train_dataset = TextDataset(training_data, tokenizer, max_seq_length=128)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer & Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)  # Use PyTorch's AdamW\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,  # Typically, warmup steps are a small fraction of total steps\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_num = epoch + 1\n",
        "        logger.info(f\"Epoch: {epoch_num}/{epochs}\")\n",
        "        print(f\"Epoch: {epoch_num}/{epochs}\")  # Print epoch progress\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = loss_function(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        logger.info(f\"Average training loss: {avg_train_loss:.3f}\")\n",
        "        print(f\"Average training loss: {avg_train_loss:.3f}\")  # Print training loss\n",
        "\n",
        "    fscore, auc, accuracy, precision, recall = evaluate_model(model, evaluation_data, tokenizer)\n",
        "    fscore = round(fscore, 3)\n",
        "    auc = round(auc, 3)\n",
        "    accuracy = round(accuracy, 3)\n",
        "    precision = round(precision, 3)\n",
        "    recall = round(recall, 3)\n",
        "\n",
        "    # Save model to path that is alphanumeric and includes number of items and accuracies in filename\n",
        "    timestamp = re.sub('\\.[0-9]*', '_', str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\", \"\")\n",
        "    training_size = \"_\" + str(len(training_data))\n",
        "    accuracies = f\"{fscore}_{auc}_{accuracy}_{precision}_{recall}\"\n",
        "\n",
        "    os.makedirs(\"models\", exist_ok=True)  # Ensure the models directory exists\n",
        "    model_path = f\"models/{timestamp}{accuracies}{training_size}.params\"\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    logger.info(f\"Model saved to: {model_path}\")\n",
        "    print(f\"Model saved to: {model_path}\")  # Print model save path\n",
        "    return model_path\n",
        "\n",
        "def evaluate_model(model, evaluation_data, tokenizer, batch_size=32):\n",
        "    \"\"\"Evaluate the model on the held-out evaluation data\n",
        "\n",
        "    Return the f-value for disaster-related and the AUC\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    dataset = TextDataset(evaluation_data, tokenizer, max_seq_length=128)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            probs_related = probabilities[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(probs_related.tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    # Calculate metrics using sklearn\n",
        "    binary_predictions = [1 if p > 0.5 else 0 for p in all_probs]  # Threshold at 0.5\n",
        "    fscore = f1_score(all_labels, binary_predictions)\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    accuracy = accuracy_score(all_labels, binary_predictions)  # Use binary predictions for accuracy\n",
        "    precision = precision_score(all_labels, binary_predictions)\n",
        "    recall = recall_score(all_labels, binary_predictions)\n",
        "\n",
        "    logger.info(f\"[fscore, auc, accuracy, precision, recall] = {fscore}, {auc},{accuracy},{precision},{recall}\")\n",
        "    print(f\"[fscore, auc, accuracy, precision, recall] = {fscore}, {auc},{accuracy},{precision},{recall}\")  # Print evaluation metrics\n",
        "    return [fscore, auc, accuracy, precision, recall]\n",
        "\n",
        "\n",
        "def get_low_conf_unlabeled(model, unlabeled_data, tokenizer, max_seq_length, number=80, limit=10000):\n",
        "    confidences = []\n",
        "    if limit == -1:  # Predicting on all data\n",
        "        logger.info(\"Get confidences for all unlabeled data (this might take a while)\")\n",
        "        print(\"Get confidences for all unlabeled data (this might take a while)\")\n",
        "    else:\n",
        "        # Only apply the model to a limited number of items\n",
        "        shuffle(unlabeled_data)\n",
        "        unlabeled_data = unlabeled_data[:limit]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for item in unlabeled_data:\n",
        "            textid = item[0]\n",
        "            if textid in already_labeled:\n",
        "                continue\n",
        "            item[3] = \"random_remaining\"\n",
        "            text = item[1]\n",
        "\n",
        "            encoded_input = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_seq_length,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            input_ids = encoded_input['input_ids'].clone().detach().to(device)\n",
        "            attention_mask = encoded_input['attention_mask'].clone().detach().to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            prob_related = probabilities[0][1].item()\n",
        "\n",
        "            if prob_related < 0.5:\n",
        "                confidence = 1 - prob_related\n",
        "            else:\n",
        "                confidence = prob_related\n",
        "\n",
        "            item[3] = \"low confidence\"\n",
        "            item[4] = confidence\n",
        "            confidences.append(item)\n",
        "\n",
        "    # Sort by confidence ascending\n",
        "    confidences.sort(key=lambda x: x[4])\n",
        "    return confidences[:number:]\n",
        "\n",
        "def get_random_items(unlabeled_data, number=10):\n",
        "    shuffle(unlabeled_data)\n",
        "    random_items = []\n",
        "    for item in unlabeled_data:\n",
        "        textid = item[0]\n",
        "        if textid in already_labeled:\n",
        "            continue\n",
        "        item[3] = \"random_remaining\"\n",
        "        random_items.append(item)\n",
        "        if len(random_items) >= number:\n",
        "            break\n",
        "\n",
        "    return random_items\n",
        "\n",
        "def get_outliers(training_data, unlabeled_data, number=10, max_iterations=1000):\n",
        "    \"\"\"Get outliers from unlabeled data in training data\n",
        "\n",
        "    Returns number outliers\n",
        "\n",
        "    An outlier is defined as the percent of words in an item in\n",
        "    unlabeled_data that do not exist in training_data\n",
        "    \"\"\"\n",
        "    outliers = []\n",
        "    total_feature_counts = defaultdict(lambda: 0)\n",
        "\n",
        "    for item in training_data:\n",
        "        text = item[1]\n",
        "        features = text.split()\n",
        "\n",
        "        for feature in features:\n",
        "            total_feature_counts[feature] += 1\n",
        "\n",
        "    iterations = 0\n",
        "    while (len(outliers) < number and iterations < max_iterations):\n",
        "        iterations += 1\n",
        "        top_outlier = []\n",
        "        top_match = float(\"inf\")\n",
        "\n",
        "        for item in unlabeled_data:\n",
        "            textid = item[0]\n",
        "            if textid in already_labeled:\n",
        "                continue\n",
        "\n",
        "            text = item[1]\n",
        "            features = text.split()\n",
        "\n",
        "            total_matches = 1  # start at 1 for slight smoothing\n",
        "            for feature in features:\n",
        "                if feature in total_feature_counts:\n",
        "                    total_matches += total_feature_counts[feature]\n",
        "\n",
        "            ave_matches = total_matches / len(features)\n",
        "            if ave_matches < top_match:\n",
        "                top_match = ave_matches\n",
        "                top_outlier = item\n",
        "\n",
        "        if not top_outlier:\n",
        "            break  # No outlier found\n",
        "\n",
        "        # Add this outlier to list and update what is 'labeled',\n",
        "        # assuming this new outlier will get a label\n",
        "        top_outlier[3] = \"outlier\"\n",
        "        outliers.append(top_outlier)\n",
        "        text = top_outlier[1]\n",
        "        features = text.split()\n",
        "        for feature in features:\n",
        "            total_feature_counts[feature] += 1\n",
        "\n",
        "    if iterations == max_iterations:\n",
        "        logger.warning(f\"Reached maximum iterations ({max_iterations}) without finding enough outliers.\")\n",
        "        print(f\"Reached maximum iterations ({max_iterations}) without finding enough outliers.\")\n",
        "\n",
        "    return outliers\n",
        "\n",
        "def main():\n",
        "\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    global training_data, training_count, evaluation_data, evaluation_count, data\n",
        "\n",
        "    if evaluation_count < minimum_evaluation_items:\n",
        "        # Keep adding to evaluation data first\n",
        "        logger.info(\"Creating evaluation data:\\n\")\n",
        "        print(\"Creating evaluation data:\\n\")  # Print action\n",
        "\n",
        "        shuffle(data)\n",
        "        needed = minimum_evaluation_items - evaluation_count\n",
        "        selected_data = data[:needed]\n",
        "        logger.info(f\"{needed} more annotations needed\")\n",
        "        print(f\"{needed} more annotations needed\")  # Print needed annotations\n",
        "\n",
        "        annotated_data = get_annotations(selected_data)\n",
        "\n",
        "        related = [item for item in annotated_data if item[2] == \"1\"]\n",
        "        not_related = [item for item in annotated_data if item[2] == \"0\"]\n",
        "\n",
        "        # Append evaluation data\n",
        "        append_data(evaluation_related_data, related)\n",
        "        append_data(evaluation_not_related_data, not_related)\n",
        "\n",
        "    elif training_count < minimum_training_items:\n",
        "        # Let's create our first training data!\n",
        "        logger.info(\"Creating initial training data:\\n\")\n",
        "        print(\"Creating initial training data:\\n\")  # Print action\n",
        "\n",
        "        shuffle(data)\n",
        "        needed = minimum_training_items - training_count\n",
        "        selected_data = data[:needed]\n",
        "        logger.info(f\"{needed} more annotations needed\")\n",
        "        print(f\"{needed} more annotations needed\")  # Print needed annotations\n",
        "\n",
        "        annotated_data = get_annotations(selected_data)\n",
        "\n",
        "        related = [item for item in annotated_data if item[2] == \"1\"]\n",
        "        not_related = [item for item in annotated_data if item[2] == \"0\"]\n",
        "\n",
        "        # Append training data\n",
        "        append_data(training_related_data, related)\n",
        "        append_data(training_not_related_data, not_related)\n",
        "    else:\n",
        "        # Let's start Active Learning!!\n",
        "\n",
        "        # Train new model with current training data\n",
        "        logger.info(\"Training model with current training data.\")\n",
        "        print(\"Training model with current training data.\")  # Print action\n",
        "\n",
        "        model_path = train_model(training_data, None, evaluation_data)\n",
        "\n",
        "        logger.info(\"Sampling via Active Learning:\\n\")\n",
        "        print(\"Sampling via Active Learning:\\n\")  # Print action\n",
        "\n",
        "        model = BERTTextClassifier(num_labels=2, max_seq_length=128).to(device)\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.to(device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model from {model_path}: {e}\")\n",
        "            print(f\"Error loading model from {model_path}: {e}\")  # Print error\n",
        "            return\n",
        "\n",
        "        # Get items per iteration with the following breakdown of strategies:\n",
        "        random_items = get_random_items(data, number=10)\n",
        "        low_confidences = get_low_conf_unlabeled(model, data, tokenizer, 128, number=80)\n",
        "        outliers = get_outliers(training_data + random_items + low_confidences, data, number=10)\n",
        "\n",
        "        sampled_data = random_items + low_confidences + outliers\n",
        "        shuffle(sampled_data)\n",
        "\n",
        "        annotated_sampled_data = get_annotations(sampled_data)\n",
        "        related = [item for item in annotated_sampled_data if item[2] == \"1\"]\n",
        "        not_related = [item for item in annotated_sampled_data if item[2] == \"0\"]\n",
        "\n",
        "        # Append training data\n",
        "        append_data(training_related_data, related)\n",
        "        append_data(training_not_related_data, not_related)\n",
        "\n",
        "    if training_count > minimum_training_items:\n",
        "        logger.info(\"\\nRetraining model with new data\")\n",
        "        print(\"\\nRetraining model with new data\")  # Print action\n",
        "\n",
        "        # UPDATE OUR DATA AND (RE)TRAIN MODEL WITH NEWLY ANNOTATED DATA\n",
        "        training_data = load_data(training_related_data) + load_data(training_not_related_data)\n",
        "        training_count = len(training_data)\n",
        "\n",
        "        evaluation_data = load_data(evaluation_related_data) + load_data(evaluation_not_related_data)\n",
        "        evaluation_count = len(evaluation_data)\n",
        "\n",
        "        logger.info(\"Training model with updated training data.\")\n",
        "        print(\"Training model with updated training data.\")  # Print action\n",
        "\n",
        "        model_path = train_model(training_data, None, evaluation_data)\n",
        "        model = BERTTextClassifier(num_labels=2, max_seq_length=128).to(device)\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.to(device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model from {model_path}: {e}\")\n",
        "            print(f\"Error loading model from {model_path}: {e}\")  # Print error\n",
        "            return\n",
        "\n",
        "        fscore, auc = evaluate_model(model, evaluation_data, tokenizer)\n",
        "        logger.info(f\"[fscore, auc] = {fscore}, {auc}\")\n",
        "        print(f\"[fscore, auc] = {fscore}, {auc}\")  # Print evaluation metrics\n",
        "        logger.info(f\"Model saved to: {model_path}\")\n",
        "        print(f\"Model saved to: {model_path}\")  # Print model save path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yQc1sBrocww3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-djbtcI6HsT"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9P-d2rPQS1J",
        "outputId": "68c8cad2-1793-4add-bc8b-e05a222e2c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "<ipython-input-13-02c529361dee>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/Bert-for-text-classification/models/20241105_204118_0.642_0.897_506.params\"))  # Replace with your model path\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence is predicted to be hatefull with confidence: 0.5089988708496094\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn # Add this import statement\n",
        "from torch.nn import functional as F # Add this import statement\n",
        "\n",
        "class BERTTextClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, max_seq_length):\n",
        "        super(BERTTextClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased') # Use BertModel instead of transformers.BertModel\n",
        "        self.dropout = nn.Dropout(0.1)  # Adjust dropout rate as needed\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        pooled_output = outputs[1] #equivalent to outputs .pooler_output\n",
        "  # Use the pooled output for classification\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Load the tokenizer and model (assuming you have downloaded and saved them)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BERTTextClassifier(num_labels=2, max_seq_length=128).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/Bert-for-text-classification/models/20241105_204118_0.642_0.897_506.params\"))  # Replace with your model path\n",
        "\n",
        "def predict(sentence):\n",
        "  \"\"\"\n",
        "  Classifies a new sentence using the loaded model.\n",
        "\n",
        "  Args:\n",
        "      sentence: The sentence to be classified (disaster-related or not).\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing the predicted label (0 or 1) and the confidence score.\n",
        "  \"\"\"\n",
        "  # Preprocess the sentence\n",
        "  input_ids, attention_mask = make_feature_vector(sentence, tokenizer, max_seq_length=128)\n",
        "\n",
        "  # Make prediction\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_ids.to(device), attention_mask.to(device))\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    prob_related = probabilities[0][1].item()\n",
        "    predicted_label = 1 if prob_related > 0.5 else 0\n",
        "    return predicted_label, prob_related\n",
        "\n",
        "# Define the function to create feature vectors (assuming the make_feature_vector function is defined elsewhere)\n",
        "def make_feature_vector(text, tokenizer, max_seq_length):\n",
        "  # Implement your feature vector creation logic here (same as the original code)\n",
        "\n",
        "  tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "  return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "# Example usage\n",
        "new_sentence = \" Trump is an orange buffoon \"\n",
        "prediction, confidence = predict(new_sentence)\n",
        "\n",
        "if prediction == 1:\n",
        "  print(\"The sentence is predicted to be hatefull with confidence:\", confidence)\n",
        "else:\n",
        "  print(\"The sentence is predicted to not be hatefull with confidence:\", confidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9pD6fFtciSz"
      },
      "source": [
        "##Saving in drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHCfWZZbb6ru",
        "outputId": "c64eb46e-8267-49f0-c359-c0756ac1cb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiMZ3MHka_Vq"
      },
      "outputs": [],
      "source": [
        "#Save the .params file from colab to gdrive\n",
        "!cp /content/Bert-for-text-classification/models/20250320_175047_0.548_0.93_0.883_0.409_0.833_400.params /content/gdrive/MyDrive/Saved_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc2eclknXJwJ"
      },
      "source": [
        "### Loading from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mT1l8hfZDt1",
        "outputId": "8fb2a6be-0190-4c34-da5e-db5cbd83a502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKHw4ZzTX8-A",
        "outputId": "07f15a12-25ce-463a-db03-48042d0f66d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i50WRwSSXOaD"
      },
      "outputs": [],
      "source": [
        "#Save the .params file from google drive to the /models directory\n",
        "\n",
        "!cp /content/drive/MyDrive/Saved_models/20250320_175047_0.548_0.93_0.883_0.409_0.833_400.params /content/Bert-for-text-classification/models/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifying a .csv file using the .params file. File must contain a 'text' column. Output: classified_texts.csv  \n"
      ],
      "metadata": {
        "id": "Lf8qJ_qmQKgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "# Step 1: Define Your Custom Model Class\n",
        "class BERTTextClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, max_seq_length=512):\n",
        "        super(BERTTextClassifier, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)  # Use the pooled output for classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Step 2: Load the Model with Your Saved Weights\n",
        "num_labels = 2  # Adjust to match your model's number of classes\n",
        "model = BERTTextClassifier(num_labels)\n",
        "model.load_state_dict(torch.load(\"/content/Bert-for-text-classification/models/20250320_175047_0.548_0.93_0.883_0.409_0.833_400.params\"))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Step 3: Define the Dataset Class to Preprocess New Data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_seq_length=512):\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "\n",
        "        for text in texts:\n",
        "            encoded_input = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_seq_length,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            self.input_ids.append(encoded_input['input_ids'].squeeze(0))\n",
        "            self.attention_masks.append(encoded_input['attention_mask'].squeeze(0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx]\n",
        "        }\n",
        "\n",
        "# Step 4: Load and Tokenize the .csv File Data\n",
        "df = pd.read_csv(\"/content/Bert-for-text-classification/data/data_for_inference/unseen_texts.csv\", header=None, names=['ID', 'text'])\n",
        "text_data = df['text']  # Replace with your actual text column name\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = TextDataset(text_data, tokenizer, max_seq_length=512)\n",
        "dataloader = DataLoader(dataset, batch_size=32)  # Use a batch size for efficiency\n",
        "\n",
        "# Step 5: Run Predictions\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())  # Collect predictions\n",
        "\n",
        "# Step 6: Save Predictions to .CSV\n",
        "df['predictions'] = predictions\n",
        "df.to_csv(\"classified_texts.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ALSXfVdOQX9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM+ERMYyqth2CylXfrIyAiU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}