{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgilos/Bert-for-text-classification/blob/main/Active%2BBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S67cOg4MtLIE",
        "outputId": "12d56604-be92-407e-9795-5be0381ef8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bert-for-text-classification'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 151 (delta 5), reused 11 (delta 4), pack-reused 137 (from 1)\u001b[K\n",
            "Receiving objects: 100% (151/151), 36.81 MiB | 20.20 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "#cloning the repository\n",
        "#https://github.com/rmunro/pytorch_active_learning.git\n",
        "#https://github.com/georgilos/Bert-for-text-classification.git\n",
        "!git clone https://github.com/georgilos/Bert-for-text-classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rkxQlxalKN8",
        "outputId": "d373d93d-810a-46f9-f4ec-d5278751f421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "active_learning_basics.py    \u001b[0m\u001b[01;34mevaluation_data\u001b[0m/     READMEa.md        uncertainty_sampling.py\n",
            "active_learning.py           LICENSE              README.md         \u001b[01;34munlabeled_data\u001b[0m/\n",
            "advanced_active_learning.py  \u001b[01;34mmodels\u001b[0m/              requirements.txt  \u001b[01;34mvalidation_data\u001b[0m/\n",
            "diversity_sampling.py        pytorch_clusters.py  \u001b[01;34mtraining_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jegydkELtWMO",
        "outputId": "d9503bc5-5c70-4b9e-8525-5e8dfc035da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Bert-for-text-classification/'\n",
            "/content/Bert-for-text-classification\n"
          ]
        }
      ],
      "source": [
        "#changing directory to repository\n",
        "#%cd pytorch_active_learning/\n",
        "%cd Bert-for-text-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3ihIG5S17HN",
        "outputId": "d9366fb6-2856-451b-83b1-da3c94a2824e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFECdj7lcz3"
      },
      "source": [
        "### Excecuting active_learning_basic.py with the SimpleTextClassifier swapped with bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4OlALQcPZvK",
        "outputId": "e44c65c2-bf1d-4381-dd79-35bc720bf0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with current training data.\n",
            "Epoch: 1/3\n",
            "Average training loss: 0.561\n",
            "Epoch: 2/3\n",
            "Average training loss: 0.368\n",
            "Epoch: 3/3\n",
            "Average training loss: 0.266\n",
            "[fscore, auc] = 0.6735751295336787, 0.8982463659416409\n",
            "Model saved to: models/20241105_203804_0.674_0.898_500.params\n",
            "Sampling via Active Learning:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-f7f140b1e5c6>:552: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "billionaire pissing contest yawning faceyawning faceyawning face\n",
            "\n",
            "> 1\n",
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "peoplecnbprolifedadsmomsprolifebynaturedadmomchildrenhomelandsafganistanitalygermanychinaetc samesexmarriages messespeople causespeopleturnin transgenderpeople wernthomosexualslesbiansortransgenderpeople wersonsdaughters\n",
            "\n",
            "> 1\n",
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "hello and i had to turn off attacking biden for not cleaning up the gop caused mess adequately with no mention of gop caused the mess\n",
            "\n",
            "> 1\n",
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "until it â€™ s stolen and pawned\n",
            "\n",
            "> \n",
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "youre joking right kids are fine we were all fine the only ones who werent were those taking meds that suppress their immune system stop acting like a chick in a nest with your mouth open blindly gobbling up the gov propaganda and record profits by pharm companies\n",
            "\n",
            "> \n",
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "nobody has time to sit through yet more videos give me a news paragraph i can skim at hyper speed old man will keep yelling at cloud\n",
            "\n",
            "> \n",
            "Please type 1 if this message is disaster-related, or hit Enter if not.\n",
            "Type 2 to go back to the last message, type d to see detailed definitions, or type s to save your annotations.\n",
            "\n",
            "now drumpf knows what the rest of think should happen to him and his magats januarythinsurrection\n",
            "\n",
            "> s\n",
            "\n",
            "Retraining model with new data\n",
            "Training model with updated training data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/3\n",
            "Average training loss: 0.558\n",
            "Epoch: 2/3\n",
            "Average training loss: 0.333\n",
            "Epoch: 3/3\n",
            "Average training loss: 0.233\n",
            "[fscore, auc] = 0.6415094339622641, 0.8966659523554413\n",
            "Model saved to: models/20241105_204118_0.642_0.897_506.params\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-f7f140b1e5c6>:592: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fscore, auc] = 0.6415094339622641, 0.8966659523554413\n",
            "[fscore, auc] = 0.6415094339622641, 0.8966659523554413\n",
            "Model saved to: models/20241105_204118_0.642_0.897_506.params\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"INTRODUCTION TO ACTIVE LEARNING\n",
        "\n",
        "A simple text classification algorithm in PyTorch\n",
        "\n",
        "This is an open source example to accompany Chapter 2 from the book:\n",
        "\"Human-in-the-Loop Machine Learning\"\n",
        "\n",
        "This example tries to classify news headlines into one of two categories:\n",
        "  disaster-related\n",
        "  not disaster-related\n",
        "\n",
        "It looks for low confidence items and outliers humans should review\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "import datetime\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "from random import shuffle\n",
        "from collections import defaultdict\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW  # Use PyTorch's AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import logging\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "__author__ = \"Robert Munro\"\n",
        "__license__ = \"MIT\"\n",
        "__version__ = \"1.0.1\"\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# settings\n",
        "\n",
        "minimum_evaluation_items = 1200  # annotate this many randomly sampled items first for evaluation data before creating training data\n",
        "minimum_training_items = 400  # minimum number of training items before we first train a model\n",
        "\n",
        "epochs = 3  # number of epochs per training session\n",
        "select_per_epoch = 200  # number to select per epoch per label\n",
        "batch_size = 16  # Batch size for training\n",
        "\n",
        "data = []\n",
        "test_data = []\n",
        "\n",
        "# directories with data\n",
        "unlabeled_data = \"unlabeled_data/unlabeled_data.csv\"\n",
        "evaluation_related_data = \"evaluation_data/related.csv\"\n",
        "evaluation_not_related_data = \"evaluation_data/not_related.csv\"\n",
        "training_related_data = \"training_data/related.csv\"\n",
        "training_not_related_data = \"training_data/not_related.csv\"\n",
        "# validation_related_data # not used in this example\n",
        "# validation_not_related_data # not used in this example\n",
        "\n",
        "already_labeled = {}  # tracking what is already labeled\n",
        "\n",
        "def load_data(filepath, skip_already_labeled=False):\n",
        "    # csv format: [ID, TEXT, LABEL, SAMPLING_STRATEGY, CONFIDENCE]\n",
        "    data = []\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            for row in reader:\n",
        "                if skip_already_labeled and row[0] in already_labeled:\n",
        "                    continue\n",
        "\n",
        "                # Ensure all necessary columns are present\n",
        "                while len(row) < 5:\n",
        "                    if len(row) == 2:\n",
        "                        row.append(\"\")  # LABEL\n",
        "                    elif len(row) == 3:\n",
        "                        row.append(\"\")  # SAMPLING_STRATEGY\n",
        "                    elif len(row) == 4:\n",
        "                        row.append(0)  # CONFIDENCE\n",
        "\n",
        "                data.append(row)\n",
        "\n",
        "                label = str(row[2])\n",
        "                if row[2] != \"\":\n",
        "                    textid = row[0]\n",
        "                    already_labeled[textid] = label\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {filepath}\")\n",
        "    return data\n",
        "\n",
        "def append_data(filepath, data):\n",
        "    \"\"\"Append data to a CSV file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'a', encoding='utf-8', errors='replace', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerows(data)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {filepath}\")\n",
        "\n",
        "def write_data(filepath, data):\n",
        "    \"\"\"Write data to a CSV file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8', errors='replace', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerows(data)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {filepath}\")\n",
        "\n",
        "# LOAD ALL UNLABELED, TRAINING, VALIDATION, AND EVALUATION DATA\n",
        "training_data = load_data(training_related_data) + load_data(training_not_related_data)\n",
        "training_count = len(training_data)\n",
        "\n",
        "evaluation_data = load_data(evaluation_related_data) + load_data(evaluation_not_related_data)\n",
        "evaluation_count = len(evaluation_data)\n",
        "\n",
        "data = load_data(unlabeled_data, skip_already_labeled=True)\n",
        "\n",
        "annotation_instructions = (\n",
        "    \"Please type 1 if this message is disaster-related, \"\n",
        "    \"or hit Enter if not.\\n\"\n",
        "    \"Type 2 to go back to the last message, \"\n",
        "    \"type d to see detailed definitions, \"\n",
        "    \"or type s to save your annotations.\\n\"\n",
        ")\n",
        "\n",
        "last_instruction = (\n",
        "    \"All done!\\n\"\n",
        "    \"Type 2 to go back to change any labels,\\n\"\n",
        "    \"or Enter to save your annotations.\"\n",
        ")\n",
        "\n",
        "detailed_instructions = (\n",
        "    \"A 'disaster-related' headline is any story about a disaster.\\n\"\n",
        "    \"It includes:\\n\"\n",
        "    \"  - human, animal and plant disasters.\\n\"\n",
        "    \"  - the response to disasters (aid).\\n\"\n",
        "    \"  - natural disasters and man-made ones like wars.\\n\"\n",
        "    \"It does not include:\\n\"\n",
        "    \"  - criminal acts and non-disaster-related police work\\n\"\n",
        "    \"  - post-response activity like disaster-related memorials.\\n\\n\"\n",
        ")\n",
        "\n",
        "def get_annotations(data, default_sampling_strategy=\"random\"):\n",
        "    \"\"\"Prompts annotator for label from command line and adds annotations to data\n",
        "\n",
        "    Keyword arguments:\n",
        "        data -- a list of unlabeled items where each item is\n",
        "                [ID, TEXT, LABEL, SAMPLING_STRATEGY, CONFIDENCE]\n",
        "        default_sampling_strategy -- strategy to use for each item if not already specified\n",
        "    \"\"\"\n",
        "\n",
        "    ind = 0\n",
        "    while ind < len(data):\n",
        "        if ind < 0:\n",
        "            ind = 0  # in case you've gone back before the first\n",
        "        if ind < len(data):\n",
        "            textid = data[ind][0]\n",
        "            text = data[ind][1]\n",
        "            label = data[ind][2]\n",
        "            strategy = data[ind][3]\n",
        "\n",
        "            if textid in already_labeled:\n",
        "                print(f\"Skipping seen label: {label}\")\n",
        "                ind += 1\n",
        "            else:\n",
        "                print(annotation_instructions)\n",
        "                label_input = input(text + \"\\n\\n> \").strip()\n",
        "\n",
        "                if label_input == \"2\":\n",
        "                    ind -= 1  # go back\n",
        "                elif label_input == \"d\":\n",
        "                    print(detailed_instructions)  # print detailed instructions\n",
        "                elif label_input == \"s\":\n",
        "                    break  # save and exit\n",
        "                else:\n",
        "                    if label_input != \"1\":\n",
        "                        label = \"0\"  # treat everything other than 1 as 0\n",
        "                    else:\n",
        "                        label = \"1\"\n",
        "\n",
        "                    data[ind][2] = label  # add label to our data\n",
        "\n",
        "                    if not data[ind][3]:\n",
        "                        data[ind][3] = default_sampling_strategy  # add default if none given\n",
        "                    ind += 1\n",
        "\n",
        "        else:\n",
        "            # Last one - give annotator a chance to go back\n",
        "            print(last_instruction)\n",
        "            label_input = input(\"\\n\\n> \").strip()\n",
        "            if label_input == \"2\":\n",
        "                ind -= 1\n",
        "            else:\n",
        "                ind += 1\n",
        "\n",
        "    return data\n",
        "\n",
        "class BERTTextClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, max_seq_length):\n",
        "        super(BERTTextClassifier, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)  # Use the pooled output for classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_seq_length):\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "        self.labels = []\n",
        "\n",
        "        for item in data:\n",
        "            try:\n",
        "                label = int(item[2])\n",
        "            except ValueError:\n",
        "                # If not, skip this item or assign a default label\n",
        "                logger.warning(f\"Skipping item with invalid label: {item[2]}\")  # Log the warning\n",
        "                print(f\"Skipping item with invalid label: {item[2]}\")  # Print to console\n",
        "                continue  # Skip this item\n",
        "            text = item[1]\n",
        "            encoded_input = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_seq_length,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            # Avoid using torch.tensor on existing tensors\n",
        "            self.input_ids.append(encoded_input['input_ids'].clone().detach().squeeze(0))\n",
        "            self.attention_masks.append(encoded_input['attention_mask'].clone().detach().squeeze(0))\n",
        "            self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "def train_model(training_data, validation_data, evaluation_data, num_labels=2):\n",
        "    \"\"\"Train model on the given training_data\n",
        "      Tune with the validation_data\n",
        "      Evaluate accuracy with the evaluation_data\n",
        "    \"\"\"\n",
        "    model = BERTTextClassifier(num_labels=num_labels, max_seq_length=128).to(device)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    train_dataset = TextDataset(training_data, tokenizer, max_seq_length=128)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer & Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)  # Use PyTorch's AdamW\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,  # Typically, warmup steps are a small fraction of total steps\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_num = epoch + 1\n",
        "        logger.info(f\"Epoch: {epoch_num}/{epochs}\")\n",
        "        print(f\"Epoch: {epoch_num}/{epochs}\")  # Print epoch progress\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = loss_function(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        logger.info(f\"Average training loss: {avg_train_loss:.3f}\")\n",
        "        print(f\"Average training loss: {avg_train_loss:.3f}\")  # Print training loss\n",
        "\n",
        "    fscore, auc = evaluate_model(model, evaluation_data, tokenizer)\n",
        "    fscore = round(fscore, 3)\n",
        "    auc = round(auc, 3)\n",
        "\n",
        "    # Save model to path that is alphanumeric and includes number of items and accuracies in filename\n",
        "    timestamp = re.sub('\\.[0-9]*', '_', str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\", \"\")\n",
        "    training_size = \"_\" + str(len(training_data))\n",
        "    accuracies = f\"{fscore}_{auc}\"\n",
        "\n",
        "    os.makedirs(\"models\", exist_ok=True)  # Ensure the models directory exists\n",
        "    model_path = f\"models/{timestamp}{accuracies}{training_size}.params\"\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    logger.info(f\"Model saved to: {model_path}\")\n",
        "    print(f\"Model saved to: {model_path}\")  # Print model save path\n",
        "    return model_path\n",
        "\n",
        "def evaluate_model(model, evaluation_data, tokenizer, batch_size=32):\n",
        "    \"\"\"Evaluate the model on the held-out evaluation data\n",
        "\n",
        "    Return the f-value for disaster-related and the AUC\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    dataset = TextDataset(evaluation_data, tokenizer, max_seq_length=128)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            probs_related = probabilities[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(probs_related.tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    # Calculate metrics using sklearn\n",
        "    fscore = f1_score(all_labels, [1 if p > 0.5 else 0 for p in all_probs])\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "    logger.info(f\"[fscore, auc] = {fscore}, {auc}\")\n",
        "    print(f\"[fscore, auc] = {fscore}, {auc}\")  # Print evaluation metrics\n",
        "    return [fscore, auc]\n",
        "\n",
        "\n",
        "def get_low_conf_unlabeled(model, unlabeled_data, tokenizer, max_seq_length, number=80, limit=10000):\n",
        "    confidences = []\n",
        "    if limit == -1:  # Predicting on all data\n",
        "        logger.info(\"Get confidences for all unlabeled data (this might take a while)\")\n",
        "        print(\"Get confidences for all unlabeled data (this might take a while)\")\n",
        "    else:\n",
        "        # Only apply the model to a limited number of items\n",
        "        shuffle(unlabeled_data)\n",
        "        unlabeled_data = unlabeled_data[:limit]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for item in unlabeled_data:\n",
        "            textid = item[0]\n",
        "            if textid in already_labeled:\n",
        "                continue\n",
        "            item[3] = \"random_remaining\"\n",
        "            text = item[1]\n",
        "\n",
        "            encoded_input = tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_seq_length,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            input_ids = encoded_input['input_ids'].clone().detach().to(device)\n",
        "            attention_mask = encoded_input['attention_mask'].clone().detach().to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            prob_related = probabilities[0][1].item()\n",
        "\n",
        "            if prob_related < 0.5:\n",
        "                confidence = 1 - prob_related\n",
        "            else:\n",
        "                confidence = prob_related\n",
        "\n",
        "            item[3] = \"low confidence\"\n",
        "            item[4] = confidence\n",
        "            confidences.append(item)\n",
        "\n",
        "    # Sort by confidence ascending\n",
        "    confidences.sort(key=lambda x: x[4])\n",
        "    return confidences[:number:]\n",
        "\n",
        "def get_random_items(unlabeled_data, number=10):\n",
        "    shuffle(unlabeled_data)\n",
        "    random_items = []\n",
        "    for item in unlabeled_data:\n",
        "        textid = item[0]\n",
        "        if textid in already_labeled:\n",
        "            continue\n",
        "        item[3] = \"random_remaining\"\n",
        "        random_items.append(item)\n",
        "        if len(random_items) >= number:\n",
        "            break\n",
        "\n",
        "    return random_items\n",
        "\n",
        "def get_outliers(training_data, unlabeled_data, number=10, max_iterations=1000):\n",
        "    \"\"\"Get outliers from unlabeled data in training data\n",
        "\n",
        "    Returns number outliers\n",
        "\n",
        "    An outlier is defined as the percent of words in an item in\n",
        "    unlabeled_data that do not exist in training_data\n",
        "    \"\"\"\n",
        "    outliers = []\n",
        "    total_feature_counts = defaultdict(lambda: 0)\n",
        "\n",
        "    for item in training_data:\n",
        "        text = item[1]\n",
        "        features = text.split()\n",
        "\n",
        "        for feature in features:\n",
        "            total_feature_counts[feature] += 1\n",
        "\n",
        "    iterations = 0\n",
        "    while (len(outliers) < number and iterations < max_iterations):\n",
        "        iterations += 1\n",
        "        top_outlier = []\n",
        "        top_match = float(\"inf\")\n",
        "\n",
        "        for item in unlabeled_data:\n",
        "            textid = item[0]\n",
        "            if textid in already_labeled:\n",
        "                continue\n",
        "\n",
        "            text = item[1]\n",
        "            features = text.split()\n",
        "\n",
        "            total_matches = 1  # start at 1 for slight smoothing\n",
        "            for feature in features:\n",
        "                if feature in total_feature_counts:\n",
        "                    total_matches += total_feature_counts[feature]\n",
        "\n",
        "            ave_matches = total_matches / len(features)\n",
        "            if ave_matches < top_match:\n",
        "                top_match = ave_matches\n",
        "                top_outlier = item\n",
        "\n",
        "        if not top_outlier:\n",
        "            break  # No outlier found\n",
        "\n",
        "        # Add this outlier to list and update what is 'labeled',\n",
        "        # assuming this new outlier will get a label\n",
        "        top_outlier[3] = \"outlier\"\n",
        "        outliers.append(top_outlier)\n",
        "        text = top_outlier[1]\n",
        "        features = text.split()\n",
        "        for feature in features:\n",
        "            total_feature_counts[feature] += 1\n",
        "\n",
        "    if iterations == max_iterations:\n",
        "        logger.warning(f\"Reached maximum iterations ({max_iterations}) without finding enough outliers.\")\n",
        "        print(f\"Reached maximum iterations ({max_iterations}) without finding enough outliers.\")\n",
        "\n",
        "    return outliers\n",
        "\n",
        "def main():\n",
        "\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    global training_data, training_count, evaluation_data, evaluation_count, data\n",
        "\n",
        "    if evaluation_count < minimum_evaluation_items:\n",
        "        # Keep adding to evaluation data first\n",
        "        logger.info(\"Creating evaluation data:\\n\")\n",
        "        print(\"Creating evaluation data:\\n\")  # Print action\n",
        "\n",
        "        shuffle(data)\n",
        "        needed = minimum_evaluation_items - evaluation_count\n",
        "        selected_data = data[:needed]\n",
        "        logger.info(f\"{needed} more annotations needed\")\n",
        "        print(f\"{needed} more annotations needed\")  # Print needed annotations\n",
        "\n",
        "        annotated_data = get_annotations(selected_data)\n",
        "\n",
        "        related = [item for item in annotated_data if item[2] == \"1\"]\n",
        "        not_related = [item for item in annotated_data if item[2] == \"0\"]\n",
        "\n",
        "        # Append evaluation data\n",
        "        append_data(evaluation_related_data, related)\n",
        "        append_data(evaluation_not_related_data, not_related)\n",
        "\n",
        "    elif training_count < minimum_training_items:\n",
        "        # Let's create our first training data!\n",
        "        logger.info(\"Creating initial training data:\\n\")\n",
        "        print(\"Creating initial training data:\\n\")  # Print action\n",
        "\n",
        "        shuffle(data)\n",
        "        needed = minimum_training_items - training_count\n",
        "        selected_data = data[:needed]\n",
        "        logger.info(f\"{needed} more annotations needed\")\n",
        "        print(f\"{needed} more annotations needed\")  # Print needed annotations\n",
        "\n",
        "        annotated_data = get_annotations(selected_data)\n",
        "\n",
        "        related = [item for item in annotated_data if item[2] == \"1\"]\n",
        "        not_related = [item for item in annotated_data if item[2] == \"0\"]\n",
        "\n",
        "        # Append training data\n",
        "        append_data(training_related_data, related)\n",
        "        append_data(training_not_related_data, not_related)\n",
        "    else:\n",
        "        # Let's start Active Learning!!\n",
        "\n",
        "        # Train new model with current training data\n",
        "        logger.info(\"Training model with current training data.\")\n",
        "        print(\"Training model with current training data.\")  # Print action\n",
        "\n",
        "        model_path = train_model(training_data, None, evaluation_data)\n",
        "\n",
        "        logger.info(\"Sampling via Active Learning:\\n\")\n",
        "        print(\"Sampling via Active Learning:\\n\")  # Print action\n",
        "\n",
        "        model = BERTTextClassifier(num_labels=2, max_seq_length=128).to(device)\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.to(device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model from {model_path}: {e}\")\n",
        "            print(f\"Error loading model from {model_path}: {e}\")  # Print error\n",
        "            return\n",
        "\n",
        "        # Get items per iteration with the following breakdown of strategies:\n",
        "        random_items = get_random_items(data, number=10)\n",
        "        low_confidences = get_low_conf_unlabeled(model, data, tokenizer, 128, number=80)\n",
        "        outliers = get_outliers(training_data + random_items + low_confidences, data, number=10)\n",
        "\n",
        "        sampled_data = random_items + low_confidences + outliers\n",
        "        shuffle(sampled_data)\n",
        "\n",
        "        annotated_sampled_data = get_annotations(sampled_data)\n",
        "        related = [item for item in annotated_sampled_data if item[2] == \"1\"]\n",
        "        not_related = [item for item in annotated_sampled_data if item[2] == \"0\"]\n",
        "\n",
        "        # Append training data\n",
        "        append_data(training_related_data, related)\n",
        "        append_data(training_not_related_data, not_related)\n",
        "\n",
        "    if training_count > minimum_training_items:\n",
        "        logger.info(\"\\nRetraining model with new data\")\n",
        "        print(\"\\nRetraining model with new data\")  # Print action\n",
        "\n",
        "        # UPDATE OUR DATA AND (RE)TRAIN MODEL WITH NEWLY ANNOTATED DATA\n",
        "        training_data = load_data(training_related_data) + load_data(training_not_related_data)\n",
        "        training_count = len(training_data)\n",
        "\n",
        "        evaluation_data = load_data(evaluation_related_data) + load_data(evaluation_not_related_data)\n",
        "        evaluation_count = len(evaluation_data)\n",
        "\n",
        "        logger.info(\"Training model with updated training data.\")\n",
        "        print(\"Training model with updated training data.\")  # Print action\n",
        "\n",
        "        model_path = train_model(training_data, None, evaluation_data)\n",
        "        model = BERTTextClassifier(num_labels=2, max_seq_length=128).to(device)\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.to(device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model from {model_path}: {e}\")\n",
        "            print(f\"Error loading model from {model_path}: {e}\")  # Print error\n",
        "            return\n",
        "\n",
        "        fscore, auc = evaluate_model(model, evaluation_data, tokenizer)\n",
        "        logger.info(f\"[fscore, auc] = {fscore}, {auc}\")\n",
        "        print(f\"[fscore, auc] = {fscore}, {auc}\")  # Print evaluation metrics\n",
        "        logger.info(f\"Model saved to: {model_path}\")\n",
        "        print(f\"Model saved to: {model_path}\")  # Print model save path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-djbtcI6HsT"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9P-d2rPQS1J",
        "outputId": "68c8cad2-1793-4add-bc8b-e05a222e2c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "<ipython-input-13-02c529361dee>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/Bert-for-text-classification/models/20241105_204118_0.642_0.897_506.params\"))  # Replace with your model path\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence is predicted to be hatefull with confidence: 0.5089988708496094\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn # Add this import statement\n",
        "from torch.nn import functional as F # Add this import statement\n",
        "\n",
        "class BERTTextClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, max_seq_length):\n",
        "        super(BERTTextClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased') # Use BertModel instead of transformers.BertModel\n",
        "        self.dropout = nn.Dropout(0.1)  # Adjust dropout rate as needed\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        pooled_output = outputs[1] #equivalent to outputs .pooler_output\n",
        "  # Use the pooled output for classification\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Load the tokenizer and model (assuming you have downloaded and saved them)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BERTTextClassifier(num_labels=2, max_seq_length=128).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/Bert-for-text-classification/models/20241105_204118_0.642_0.897_506.params\"))  # Replace with your model path\n",
        "\n",
        "def predict(sentence):\n",
        "  \"\"\"\n",
        "  Classifies a new sentence using the loaded model.\n",
        "\n",
        "  Args:\n",
        "      sentence: The sentence to be classified (disaster-related or not).\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing the predicted label (0 or 1) and the confidence score.\n",
        "  \"\"\"\n",
        "  # Preprocess the sentence\n",
        "  input_ids, attention_mask = make_feature_vector(sentence, tokenizer, max_seq_length=128)\n",
        "\n",
        "  # Make prediction\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_ids.to(device), attention_mask.to(device))\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    prob_related = probabilities[0][1].item()\n",
        "    predicted_label = 1 if prob_related > 0.5 else 0\n",
        "    return predicted_label, prob_related\n",
        "\n",
        "# Define the function to create feature vectors (assuming the make_feature_vector function is defined elsewhere)\n",
        "def make_feature_vector(text, tokenizer, max_seq_length):\n",
        "  # Implement your feature vector creation logic here (same as the original code)\n",
        "\n",
        "  tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "  return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "# Example usage\n",
        "new_sentence = \" Trump is an orange buffoon \"\n",
        "prediction, confidence = predict(new_sentence)\n",
        "\n",
        "if prediction == 1:\n",
        "  print(\"The sentence is predicted to be hatefull with confidence:\", confidence)\n",
        "else:\n",
        "  print(\"The sentence is predicted to not be hatefull with confidence:\", confidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9pD6fFtciSz"
      },
      "source": [
        "##Saving in drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHCfWZZbb6ru",
        "outputId": "ce62326f-290b-4cc5-d1ef-8990fafe1c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XiMZ3MHka_Vq"
      },
      "outputs": [],
      "source": [
        "#Save the .params file from colab to gdrive\n",
        "!cp /content/Bert-for-text-classification/models/20241105_204118_0.642_0.897_506.params /content/gdrive/MyDrive/Saved_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc2eclknXJwJ"
      },
      "source": [
        "### Loading from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mT1l8hfZDt1",
        "outputId": "05318e0c-b579-470e-ece7-6d56a710df43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i50WRwSSXOaD"
      },
      "outputs": [],
      "source": [
        "#Save the .params file from google drive to the /models directory\n",
        "\n",
        "!cp /content/gdrive/MyDrive/Saved_models/20241010_210511_0.688_0.953_989.params /content/pytorch_active_learning/models/\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM7nZZOOOacBeaBDO+aA6Pv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}